# Top Level Config Template for both the training and metric experiments

# Where to put the subconfigs
config_dir: configs

# Where (and if) to save results locally
save_dir: results_pets
local_save: False

# Where to find the slurm template, if set to None will pick up jobscript from src/locomoset/config
# slurm_template_path: None
slurm_template_name: 'jobscript_template.sh'

# Huggingface model names or paths to local models
models:
  - facebook/deit-tiny-patch16-224
  - facebook/deit-small-patch16-224

# HuggingFace Datasets or paths to local datasets
dataset_names: pcuenq/oxford-pets

# Random seeds to use
random_states:
  - 42
  - 43

# Weights and biases
wandb_args:
  entity: arc
  project: locomoset_metrics_tests
  log_model: checkpoint

# Baskerville arguments, with separate arguments available for the different experiment types
use_bask: False
bask:
  metrics:
    job_name: 'locomoset_metric_experiment'
    walltime: '0-0:30:0'
    node_number: 1
    gpu_number: 1
    cpu_per_gpu': 36
  train:
    job_name: 'locomoset_train_experiment'
    walltime: '0-0:30:0'
    node_number: 1
    gpu_number: 1
    cpu_per_gpu': 36

# Cache locations:
caches:
  datasets: null
  models: null
  preprocess_cache: disk

# Dataset specific arguments ---------------------------------------------------------------

# dataset arguments
dataset_args:
  # Name of dataset splits to train, evaluate, and test on. If one or both of val and test do
  # not exist, they will be created from the train set, using val_size and test_size. val_size
  # and test_size are ignored if they do exist (and can also be set to null if you wish to pass
  # None)
  train_split: train
  val_split: validation
  test_split: test
  val_size: 0.15
  test_size: 0.15

# list of label keeps
keep_labels:
  - null # equivalent to saying keep all labels
  - ["British Shorthair", "Persian"] # keep these two labels

# list of keep sizes (if float, 1-keep will be dropped from train and val)
keep_sizes:
  - null # equivalent to dropping nothing
  - 0.5 # keep 50% of data (AFTER dropping labels)

# Training specific config -----------------------------------------------------------------

# Arguments for transformers.TrainingArguments
training_args:
  output_dir: output
  overwrite_output_dir: True
  save_strategy: "no"
  num_train_epochs: 1
  use_mps_device: False
  evaluation_strategy: steps
  eval_steps: 100
  logging_strategy: steps

# Metric Experiment specific config ---------------------------------------------------------

# Metrics to compute
metrics:
  - renggli
  - n_pars
  - LogME

# kwargs to metrics - if not null, nested list of metric_name: kwarg1: value
metric_kwargs: null

# Name of dataset split to compute metrics for (Currently must be the same for all datasets)
dataset_splits:
  "train"

#Â No. of images to compute the metrics with
n_samples:
  50

# Arguments required for inference
inference_args:
  device: cuda
