{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to extract the time taken for training and metric computation times, this should be simply done by from the way that Phil's code pull's the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "import wandb\n",
    "from constants import DATASET_NAMES, MAX_SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "api = wandb.Api()\n",
    "datasets = list(MAX_SIZES.keys())\n",
    "train_data = utils.get_data(api, \"train\", datasets)\n",
    "metric_data = utils.get_data(api, \"metrics\", datasets)\n",
    "preproc_data = utils.get_data(api, \"preproc\", datasets)\n",
    "data = metric_data.merge(train_data, on=[\"dataset_name\", \"model_name\", \"n_samples\"])\n",
    "data = data.merge(preproc_data, on=[\"dataset_name\", \"model_name\", \"n_samples\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract timings\n",
    "data[\"LogME\"] = [v[\"time\"] for v in data[\"LogME\"].values]\n",
    "data[\"renggli\"] = [v[\"time\"] for v in data[\"renggli\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add inference and pre processing times\n",
    "data[\"LogME\"] = data[\"LogME\"] + data[\"inference_times\"]\n",
    "data[\"renggli\"] = data[\"renggli\"] + data[\"inference_times\"]\n",
    "data[\"train_runtime\"] = (\n",
    "    data[\"train_runtime\"] + data[\"preproc_times\"] + data[\"eval_runtime\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data = data.groupby([\"dataset_name\", \"n_samples\", \"n_metric_samples\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_timings(data, name):\n",
    "    \"\"\"Extract the minimum and maximum metric computation times\n",
    "\n",
    "    Args:\n",
    "        data: sub dataframe table, containing results for one combination of\n",
    "                (dataset, n_samples, n_metric_samples)\n",
    "        name: labels for this set\n",
    "    \"\"\"\n",
    "    min_train_time = data[\"train_runtime\"].min() / 60\n",
    "    max_train_time = data[\"train_runtime\"].max() / 60\n",
    "    avg_train_time = data[\"train_runtime\"].mean()\n",
    "\n",
    "    metric_times = []\n",
    "    metric_times.append(data[\"LogME\"].values)\n",
    "    metric_times.append(data[\"renggli\"].values)\n",
    "    min_metric_time = np.min(metric_times) / 60\n",
    "    max_metric_time = np.max(metric_times) / 60\n",
    "    avg_metric_time = np.mean(metric_times)\n",
    "\n",
    "    row = {\n",
    "        \"dataset\": DATASET_NAMES[name[0]],\n",
    "        \"n_samples\": name[1],\n",
    "        \"train_time\": f\"{np.around(min_train_time)} - {np.around(max_train_time)}\",\n",
    "        \"n_metric_samples\": name[2],\n",
    "        \"metric_time\": f\"{np.around(min_metric_time)} - {np.around(max_metric_time)}\",\n",
    "        \"metric_advantage\": np.around(avg_train_time / avg_metric_time),\n",
    "    }\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "def get_timings(group_data):\n",
    "    return pd.DataFrame([extract_timings(data, name) for name, data in group_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "timings = get_timings(group_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_timings = timings[timings[\"n_samples\"] == timings[\"n_metric_samples\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_timings = timings[timings[\"n_samples\"].isin(MAX_SIZES.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_training_samps_timing = timings[timings[\"n_samples\"].isin(list(MAX_SIZES.values()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_timings.to_csv(\"timings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "locomoset-yXOa9fJX-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
